{
  "hash": "fc8178db4028b0dee2bd0e3927ee8e68",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: The ERA5 tour (part 1)\nauthor: Mark Cherrie\ndate: '2024-09-10'\ncategories:\n  - data\n---\n\n![marko swift](era5tour.jpeg)\n\nERA5 deserves the same recognition as Miss Swift IMO. \n\n## So what is ERA5? \n\nERA5 is short for ECMWF Reanalysis v5 and is a climate data product from the Copernicus Climate Change Service (C3S) at the European Centre for Medium-Range Weather Forecasts (ECMWF). It's a picture of past weather using a wide range of data sources via numerical weather prediction (NWP) models. OK, enough acronyms for now.\n\nThe main **WOW** factor that ERA5 has over other datasets is it's incredible spatial and temporal resolution. The data begins 4 months into World War 2 (January 1940) and goes to present day, for every **hour**, for every 31km section of land and sea across the **globe**. \n\nIn terms of the climate data there is a wide selection at a single atmospheric level and a few at multi-level (e.g. pressure, temperature). Popular single level data includes:\n\n* 10m u-component of wind (velocity in the East direction)\n* 10m v-component of wind (velocity in the North direction)\n* Mean wave period\n* Significant height of combined wind waves and swell\n* 2m temperature\n* Total precipitation\n\n## What can you do with this data? \n\nThere are so many potential applications of this data from weather impacts on renewable energy project development (link) and [public health](https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/met.2122), alongside the more standard meterological questions of \nassessing flood risk, climate change etc.  \n\n## How do you access this data?\n\nThere are three main ways to access the data that I know about:\n\n* Climate Data Store - Copernicus API   (CDS API)\n* S3 storage (GCP/AWS)\n* Google Earth Engine (GEE) \n\n![Figure 1: Sources of ERA5 data](sources.png)\n\nDeciding on which source you use depends on how much data you need and what variables you need. If you need less than a year of any variable go for CDS API; if you need more than a year and any variable email for a bulk download; if you need over a year for a selection of variables then you might be able to use data on AWS/GCP. The Google Earth Engine option can do both, but comes at a high cost for commercial applications; there's also a good python-based tutorial [here](https://github.com/jwagemann/era5_in_gee).  \n\nIn the next couple sections I will explain how to obtain credentials, make a request for a portion of ERA5 data and then process in python. \n\n### CDS API\n\nThe standard way to interact with ERA5 data and one that does a great job for smaller datasets (most likely less than a year).\n\n\n#### Credentials\n\nCreate an account on cds - go to this [page](https://cds.climate.copernicus.eu/api-how-to) - follow the instructions and create the $HOME/.cdsapirc file.\n\n\n#### Data Request\n\nFrom here you want to generate the API request, the website will build this for you if use the select boxes :\n\n\n{{< video cdsapi.mp4 >}}\n\n\n\nNow copy and paste that into a python script - this will download the data into a netcdf called \"download.nc\": \n\n::: {#3a798ea7 .cell execution_count=1}\n``` {.python .cell-code}\nimport cdsapi\n\nc = cdsapi.Client()\n\nc.retrieve(\n    'reanalysis-era5-single-levels',\n    {\n        'product_type': 'reanalysis',\n        'format': 'netcdf',\n        'year': '2024',\n        'month': '09',\n        'variable': [\n            '10m_u_component_of_wind', '10m_v_component_of_wind',\n        ],\n        'day': [\n            '01', '02', '03',\n            '04', '05',\n        ],\n        'time': [\n            '00:00', '01:00', '02:00',\n            '03:00', '04:00', '05:00',\n            '06:00', '07:00', '08:00',\n            '09:00', '10:00', '11:00',\n            '12:00', '13:00', '14:00',\n            '15:00', '16:00', '17:00',\n            '18:00', '19:00', '20:00',\n            '21:00', '22:00', '23:00',\n        ],\n        'anon_user_timestamp': '2024-09-11 08:09:06',\n    },\n    'download.nc')\n```\n:::\n\n\n#### Processing\n\nDownload HDF5 libraries for your Linux distribution, then pip install the netCDF4 package. \n\n::: {#f7829a92 .cell execution_count=2}\n``` {.python .cell-code}\nimport netCDF4 as nc\n\n# Open the NetCDF file\nfile_path = 'download.nc'\ndataset = nc.Dataset(file_path, 'r')\n\n# Print the dataset information\nprint(dataset)\n\n# Extract a variable (e.g., temperature)\ntemperature = dataset.variables['temperature'][:]\n\n# Print the variable information\nprint(temperature)\n\n# Close the dataset\ndataset.close()\n```\n:::\n\n\n### Analysis-Ready, Cloud Optimized ERA5 (hosted on GCP)\n\n////////// insert some details /////////\n\n#### Credentials\nThe data is public so no need for credentials. \n\n#### Data Request\n\nPip install the xarray, zarr, fsspe and gcsfs packages. \n\nFollow the [tutorial](https://github.com/google-research/arco-era5/blob/main/docs/0-Surface-Reanalysis-Walkthrough.ipynb). \n\n#### Processing\n\nThe data is in Zarr format.... advantages etc.\n\n\n### ERA5 forcing data for the Weather Research and Forecasting (WRF) model (hosted on AWS)\n\nGlobal timeseries at 0.25 degree resolution from 2000-01 onwards in CSV format, for four variables:\n\n* 100-m wind speed\n* 100-m wind direction\n* 2-m temperature\n* surface pressure\n\n#### Credentials\nIt's a public bucket again, so no credentials needed, however you need to either add configuration details to ~/.aws/credentials **or** add config as the following way when creating the S3 client:\n\n::: {#591ed7e1 .cell execution_count=3}\n``` {.python .cell-code}\nfrom botocore import UNSIGNED\nfrom botocore.client import Config\nimport boto3\n\ns3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n```\n:::\n\n\n#### Data request\n\nFollow this [tutorial](https://github.com/moptis/era5-for-wrf/blob/main/tutorials/era5_global_timeseries.ipynb). In summary you want to find the gid (1 to 1038240) that matched with the latitude/longitude pair that you are interested in and then use this code:\n\n::: {#2011a46e .cell execution_count=4}\n``` {.python .cell-code}\nimport pandas as pd\nfrom io import StringIO\n\n# example GID\ngid = \"1038240\"\n\n# Define the S3 bucket and file path\nbucket_name = 'era5-for-wrf'\nfile_key = f'global_single_level/cells/{gid}/timeseries.csv'\n\n# Get the file object\ns3_object = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n\n# Read the file content\nfile_content = s3_object['Body'].read().decode('utf-8')\n\n# Use StringIO to convert the file content to a file-like object\ncsv_string_io = StringIO(file_content)\n\n# Read the CSV file into a Pandas DataFrame\nera5_df = pd.read_csv(csv_string_io, index_col = 0, skiprows = 1, parse_dates = True)\n```\n:::\n\n\n#### Processing\n\nThe data per point is a CSV with datetime as the index and then the four variables above, simple to work with in pandas or whatever data package you want to use. \n\n\n##  What next?\n\nERA6 will have some advances over ERA5, including improvements:\n\n* resolution of at least 18 km,\n* model bias,\n* realism of near-surface quantities,\n* ocean wave physics\n\n... but won't be available until 2027. So there's still life in ERA5 for some time yet, but some things are changing. \n\nThe current instance of CDS API will be decommissioned on 26 September 2024 and will no longer be accessible from this date onwards. \n\nPart 2 of this blog will summarise how to get access via the new CDS route. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}