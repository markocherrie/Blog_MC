{
  "hash": "cfb60f45c2e3c6c3e2d1a5631cfc28f9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: The ERA5 tour (part 1)\nauthor: Mark Cherrie\nfilters:\n  - social-share\ndate: '2024-09-12'\ncategories:\n  - data\nshare:\n  permalink: 'https://markocherrie.github.io/Blog_MC/posts/era5tour/'\n  description: The ERA5 tour (part 1)\n  twitter: true\n  facebook: true\n  reddit: true\n  linkedin: true\n  email: true\n  mastodon: true\nformat:\n  html:\n    code-fold: true\n    code-summary: Show the code\n---\n\n![](era5tour.jpeg)\n\n{{< fa fire >}} ERA5 deserves the same recognition as Miss Swift IMO \n\n## What is ERA5? \n\nERA5 is short for ECMWF Reanalysis v5 and is a climate data product from the Copernicus Climate Change Service (C3S) at the European Centre for Medium-Range Weather Forecasts (ECMWF). It's the best picture of past weather that we have, created using a wide range of data sources via numerical weather prediction (NWP) models. OK, enough acronyms for now.\n\nThe main **WOW** factor that ERA5 has over other datasets is it's incredible spatial and temporal resolution. The data begins 4 months into World War 2 (January 1940) and goes to present day, for every **hour**, for every 31km section of land and sea across the **globe**. \n\nIn terms of the climate data there is a wide selection at a single atmospheric level and a few at multi-level (e.g. pressure, temperature). Popular single level data includes:\n\n* 10m u-component of wind (velocity in the East direction)\n* 10m v-component of wind (velocity in the North direction)\n* Mean wave period\n* Significant height of combined wind waves and swell\n* 2m temperature\n* Total precipitation\n\n## What can you do with this data? \n\nThere are so many potential applications of this data from [wind power simulation](https://www.sciencedirect.com/science/article/pii/S0360544221017680) and [public health](https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/met.2122), alongside the more standard meterological questions of assessing flood risk, climate change etc.  \n\n## How do you access this data?\n\nThere are four main ways to access the data that I know about:\n\n* Climate Data Store - Copernicus API   (CDS API)\n* Analysis-Ready, Cloud Optimized ERA5 (hosted on GCP)\n* ERA5 forcing data for the Weather Research and Forecasting (WRF) model (hosted on AWS)\n* Google Earth Engine (GEE) \n\n![Figure 1: Sources of ERA5 data](sources2.png)\n\nDeciding on which source you use depends on how much data you need and what variables you need. If you need less than a year of any variable go for CDS API; if you need more than a year and any variable email for a bulk download; if you need over a year for a selection of variables then you might be able to use data on AWS/GCP. The Google Earth Engine option can do both, but comes at a high cost for commercial applications; if you are working in a university and already familiar with GEE then a good python-based tutorial is available [here](https://github.com/jwagemann/era5_in_gee).  \n\nIn the next couple sections I will explain how to setup your working environment, make a request for a portion of ERA5 data and then process in python. \n\n### CDS API\n\nThe CDS API is the standard way to interact with ERA5 data and one that does a great job for smaller datasets (most likely less than a year).\n\n\n#### Setup\n\nCreate an account on CDS by going to this [page](https://cds.climate.copernicus.eu/api-how-to) - follow the instructions and create the $HOME/.cdsapirc file. Pip install cdsapi.\n\nTo work with the outputs you might need to download HDF5 libraries (for me on mac it was 'brew install hdf5'). Once that's set up pip install the netCDF4 package.\n\n\n#### Data Request\n\nFrom here you want to generate the API request, the website will build this for you if use the select boxes, see below for my wonderful clicking and scrolling skills :\n\n\n{{< video cdsapi.mp4 >}}\n\n\n\nNow copy and paste that into a python script - this will download the data into a netcdf called \"download.nc\": \n\n::: {#8ef62d17 .cell execution_count=1}\n``` {.python .cell-code}\nimport cdsapi\n\nc = cdsapi.Client()\n\nc.retrieve(\n    'reanalysis-era5-single-levels',\n    {\n        'product_type': 'reanalysis',\n        'format': 'netcdf',\n        'year': '2024',\n        'month': '09',\n        'variable': [\n            '10m_u_component_of_wind', '10m_v_component_of_wind',\n        ],\n        'day': [\n            '01', '02', '03',\n            '04', '05',\n        ],\n        'time': [\n            '00:00', '01:00', '02:00',\n            '03:00', '04:00', '05:00',\n            '06:00', '07:00', '08:00',\n            '09:00', '10:00', '11:00',\n            '12:00', '13:00', '14:00',\n            '15:00', '16:00', '17:00',\n            '18:00', '19:00', '20:00',\n            '21:00', '22:00', '23:00',\n        ],\n        'anon_user_timestamp': '2024-09-11 08:09:06',\n    },\n    'download.nc')\n```\n:::\n\n\n#### Processing\n\nThe data is netcdf - a common file type for multidimensional array data. When I say multidimensional, what I'm generally thinking of is 4 dimensions - latitude, longitude, time and **some interesting data**. A key feature of netcdf is that metadata is embedded within the file. When you print the dataset in python you will get to see all this useful information. \n\nHere is an example of working with the netcdf file:\n\n::: {#a0ad329a .cell execution_count=2}\n``` {.python .cell-code}\nimport netCDF4 as nc\n\n# Open the NetCDF file\nfile_path = 'download.nc'\ndataset = nc.Dataset(file_path, 'r')\n\n# Print the dataset information\nprint(dataset)\n\n# Extract a variable (e.g., temperature)\ntemperature = dataset.variables['temperature'][:]\n\n# Print the variable information\nprint(temperature)\n\n# Close the dataset\ndataset.close()\n```\n:::\n\n\n### Analysis-Ready, Cloud Optimized ERA5 (hosted on GCP)\n\nARCO for short, this has been created by Google to feed into a weather forecasting model. There are five datasets:\n\n* 'gcp-public-data-arco-era5/co/model-level-moisture.zarr',\n* 'gcp-public-data-arco-era5/co/model-level-wind.zarr',\n* 'gcp-public-data-arco-era5/co/single-level-forecast.zarr',\n* 'gcp-public-data-arco-era5/co/single-level-reanalysis.zarr',\n* 'gcp-public-data-arco-era5/co/single-level-surface.zarr'\n\nThe reanalysis is the one we are interested in. \n\n#### Setup\n\nThe data is public so no need for credentials. \n\nPip install the xarray, zarr, fsspec and gcsfs packages. \n\n#### Data Request\n\nFollow this [tutorial](https://github.com/google-research/arco-era5/blob/main/docs/0-Surface-Reanalysis-Walkthrough.ipynb). \n\nOne of the challenging aspects of accessing via this route is that the spatial dimention corresponds to a Gaussian Grid - in particular the reduced Gaussian Grid N320 (which means there are 320 latitude points from the equator to each pole). If you want to subset to a particular lat and long within the Gaussian Grid then you need to use the \"GRIB_paramId\" attribute, this will related to the values [here](https://confluence.ecmwf.int/display/FCST/Gaussian+grid+with+320+latitude+lines+between+pole+and+equator).\n\nOtherwise the recommended way is to subset the data using the latitude and longitude attributes. **Note** that the coordinate system for ERA5 is 0 to 360 not -180 to 180. \n\n#### Processing\n\nZarr is a powerful way to store chunked, compressed, N-dimensional arrays based on an open-source specification. The key part is that it is optimised for cloud-native storage and for parallel input/output.  \n\nThere's a bit of a learning curve to working with xarray (and dask) in python.   \n\n\n### ERA5 forcing data for the Weather Research and Forecasting (WRF) model (hosted on AWS)\n\nThis source is focused on the wind industry. It's a subset of ERA5 timeseries data at 0.25 degree resolution from 2000-01 onwards in CSV format, for four variables:\n\n* 100-m wind speed\n* 100-m wind direction\n* 2-m temperature\n* surface pressure\n\n#### Setup\n\nPip install boto3 and pandas.\n\nIt's a public bucket again, so no credentials needed, however you need to either add configuration details to ~/.aws/credentials **or** add config as the following way when creating the S3 client:\n\n::: {#ccd665bf .cell execution_count=3}\n``` {.python .cell-code}\nfrom botocore import UNSIGNED\nfrom botocore.client import Config\nimport boto3\n\ns3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n```\n:::\n\n\n#### Data request\n\nFollow this [tutorial](https://github.com/moptis/era5-for-wrf/blob/main/tutorials/era5_global_timeseries.ipynb). In summary you want to find the gid (1 to 1038240) that matches with the latitude/longitude pair that you are interested in and then do something similar to below:\n\n::: {#4ed0d53a .cell execution_count=4}\n``` {.python .cell-code}\nimport pandas as pd\nfrom io import StringIO\n\n# example GID\ngid = \"1038240\"\n\n# Define the S3 bucket and file path\nbucket_name = 'era5-for-wrf'\nfile_key = f'global_single_level/cells/{gid}/timeseries.csv'\n\n# Get the file object\ns3_object = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n\n# Read the file content\nfile_content = s3_object['Body'].read().decode('utf-8')\n\n# Use StringIO to convert the file content to a file-like object\ncsv_string_io = StringIO(file_content)\n\n# Read the CSV file into a Pandas DataFrame\nera5_df = pd.read_csv(csv_string_io, index_col = 0, skiprows = 1, parse_dates = True)\n```\n:::\n\n\n#### Processing\n\nThe data per point is a CSV with datetime as the index and then the four variables above - simple to work with in pandas or whatever data package you want to use. \n\n## Summary\n\nHere is the following spiciness (read difficulty) for accessing each of the data sources:\n\n| Source | Setup | Data Request | Processing\n|------|------|------|------|\n| CDS API   |  {{< fa pepper-hot >}}{{< fa pepper-hot >}}   |  {{< fa pepper-hot >}}    |  {{< fa pepper-hot >}}   |\n| Analysis-Ready, Cloud Optimized ERA5 (hosted on GCP)    |  {{< fa pepper-hot >}}    |   {{< fa pepper-hot >}}{{< fa pepper-hot >}}  |  {{< fa pepper-hot >}}{{< fa pepper-hot >}}{{< fa pepper-hot >}}   |\n| ERA5 forcing data for the Weather Research and Forecasting (WRF) model (hosted on AWS)    |  {{< fa pepper-hot >}}    |   {{< fa pepper-hot >}}   |   {{< fa pepper-hot >}}   |\n\n: Data Access Spiciness {#tbl-letters}\n\n##  What next?\n\nHaving spent the blog post raving about ERA5 you'll be surprised to hear that their sibling has been planned and will have some major improvements:\n\n* resolution of at least 18 km,\n* model bias,\n* realism of near-surface quantities,\n* ocean wave physics\n\n... so why the fuck have I been reading about accessing an inferior data product I hear you say\n\nWell, ERA6 is not due until 2027, so there's still plenty of life in ERA5 for some time yet.\n\nIn the meantime the current instance of CDS API will be decommissioned on 26 September 2024 and will no longer be accessible from this date onwards so for part 2 of this blog I'll be updating that part and actually showing you some interesting things that you can do with the data!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}